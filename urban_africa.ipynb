{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urban Africa\n",
    "\n",
    "This notebook uses the Africapolis 2015 data to append a csv dataset containing longitudes and latitudes with a new attribute specifying whether each long/lat pair lies in an urban area or not. This task is completed by checking whether each point lies inside any of the polygons defining the borders of African cities, given by the Africapolis dataset: https://africapolis.org/data This dataset is automatically downloaded if it is not already present in the specified directory.\n",
    "\n",
    "Depending on the size of the dataset and your processing power, this can take some time. If you are using Google colab, it may not be worth utilising the multiprocessing option, as by default you will only have 2 CPUs. However, if the dataset is very large, this may still be better than just one CPU. Through experimenting with my own laptop (12 cores) it seems that using multiprocessing with 6 processors reduces runtime by a lot for a relatively small dataset (~5000 entries) from 2m 48s to 1m 30s, so for large datasets it is likely worth using. Using a few more or less makes little difference, so I have made the default half of the available processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oD_8IZ3HniLY"
   },
   "outputs": [],
   "source": [
    "## If using Google Colab, run this block to access data in your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1E4sT2ALoHHo"
   },
   "outputs": [],
   "source": [
    "## If using Google Colab, these packages will need to be installed\n",
    "!pip install geopandas\n",
    "!pip install pdl\n",
    "## If not, then any packages you have not installed will need to be installed, so include them as needed\n",
    "#!pip install pandas\n",
    "#!pip install tqdm\n",
    "#!pip install dask\n",
    "#!pip install multiprocessing\n",
    "#!pip install numpy\n",
    "#!pip install os\n",
    "#!pip install sys\n",
    "#!pip install time\n",
    "#!pip install threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjxdoT6mnwr8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tqdm\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from pdl import pdl\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the application\n",
    "\n",
    "This block will begin running the application. You will be prompted to provide file paths and other necessary information. The data will then be processed, and saved to the given file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "CERDP3vboVCf",
    "outputId": "5a33fcc7-c621-40b2-862f-118a6b708893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the directory containing your dataset, \n",
      "eg. \"C://Users/Username/Desktop/Data/:\" on local machine, or \"/content/gdrive/My Drive/Project/Data\" on Google Colab.\n",
      "C://Users/ewand/Downloads\n",
      "Please enter the name of your data file (csv), \n",
      "eg. \"data.csv\":\n",
      "data_isurban.csv\n",
      "To speed up computation for large datasets\n",
      "multiprocessing can be utilised. Should this be done? (y/n)\n",
      "y\n",
      "How many cores should be used? If left blank half will be used.\n",
      "4\n",
      "Loading Data \n",
      "Dataset doesn't contain column named longitude. Please enter the name of longitude column.\n",
      "ubefi\n",
      "\bStarting processing...\n",
      "[########################################] | 100% Completed |  1min 34.7s\n",
      "Saving Data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class SpinnerThread(threading.Thread):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(target=self._spin)\n",
    "        self._stopevent = threading.Event()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stopevent.set()\n",
    "\n",
    "    def _spin(self):\n",
    "\n",
    "        while not self._stopevent.isSet():\n",
    "            for t in '|/-\\\\':\n",
    "                sys.stdout.write(t)\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(0.5)\n",
    "                sys.stdout.write('\\b')\n",
    "\n",
    "\n",
    "def containment_tests(data, shapes, long_name='longitude', lat_name='latitude'):\n",
    "  spinner_thread = SpinnerThread()\n",
    "  spinner_thread.start()\n",
    "  data = pd.DataFrame(data)\n",
    "  points = gpd.GeoDataFrame(data.loc[:,[long_name,lat_name]], geometry=gpd.points_from_xy(data.loc[:,long_name], data.loc[:,lat_name])) #create a series of point objects representing location of events\n",
    "  polys = shapes.geometry #This is a series of polygons\n",
    "  containment_checker = polys.geometry.buffer(0).contains\n",
    "  tqdm.tqdm.pandas(position=0, leave=True)\n",
    "  spinner_thread.stop()\n",
    "  r = points.geometry.progress_apply(containment_checker)\n",
    "  return r.any(axis=1)\n",
    "\n",
    "def multi_process_containment_tests(data, shapes, long_name='longitude', lat_name='latitude', cores=int(np.round(multiprocessing.cpu_count()/2))):\n",
    "  spinner_thread = SpinnerThread()\n",
    "  spinner_thread.start()\n",
    "  data = pd.DataFrame(data)\n",
    "  \n",
    "  points = gpd.GeoDataFrame(data.loc[:,[long_name,lat_name]], geometry=gpd.points_from_xy(data.loc[:,long_name], data.loc[:,lat_name])) #create a series of point objects representing location of events\n",
    "  polys = shapes.geometry #This is a series of polygons\n",
    "  containment_checker = polys.geometry.buffer(0).contains\n",
    "  spinner_thread.stop()\n",
    "  with ProgressBar():  \n",
    "    r = dd.from_pandas(points.geometry, npartitions=cores).map_partitions(lambda dframe: pd.Series(np.any(dframe.apply(containment_checker), axis=1)), meta=pd.Series(dtype=bool)).compute(scheduler='processes')  \n",
    "  return r\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  def yes_no(question):\n",
    "    yes = set(['yes','y'])\n",
    "    no = set(['no','n'])\n",
    "     \n",
    "    while True:\n",
    "        choice = input(question).lower()\n",
    "        if choice in yes:\n",
    "           return True\n",
    "        elif choice in no:\n",
    "           return False\n",
    "        else:\n",
    "           print('Please respond with y/n.')\n",
    "  \n",
    "  def get_directory(question, error_message, prev_path=''):\n",
    "    dir_ = input(question)\n",
    "    dir_ = os.path.join(prev_path, dir_)\n",
    "    if os.path.exists(dir_):\n",
    "      return dir_\n",
    "    else:\n",
    "      print(error_message)\n",
    "      dir_ = get_directory(question, error_message)\n",
    "      return dir_\n",
    "          \n",
    "  data_dir = get_directory('Please enter the directory containing your dataset, \\neg. \"C://Users/Username/Desktop/Data/:\" on local machine, or \"/content/gdrive/My Drive/Project/Data\" on Google Colab.\\n',\n",
    "                           \"That path doesn't exist, please enter the correct path.\")\n",
    "\n",
    "  data_filename = get_directory('Please enter the name of your data file (csv), \\neg. \"data.csv\":\\n',\n",
    "                               \"The file you have given does not exist in the specified directory. Please check the details given and start again if necessary.\",\n",
    "                               data_dir)\n",
    "  multiprocess = yes_no('To speed up computation for large datasets\\nmultiprocessing can be utilised. Should this be done? (y/n)\\n')\n",
    "  \n",
    "  if multiprocess:\n",
    "    cores = input('How many cores should be used? If left blank half will be used.\\n')\n",
    "    if cores == '':\n",
    "      cores = int(np.round(multiprocessing.cpu_count()/2))\n",
    "    else: cores = int(cores)\n",
    "      \n",
    "  def load_data():\n",
    "    global africapolis\n",
    "    try: \n",
    "      africapolis = gpd.read_file(os.path.join(data_dir, 'africapolis.shp'))\n",
    "    except Exception:\n",
    "      africapolis_url = 'http://www.africapolis.org/download/Africapolis_2015_shp.zip'\n",
    "      pdl.download(africapolis_url, data_dir=data_dir, keep_download=False, overwrite_download=True, verbose=True)\n",
    "      africapolis = gpd.read_file(os.path.join(data_dir,'africapolis.shp'))\n",
    "\n",
    "  print('Loading Data ')\n",
    "  task = threading.Thread(target=load_data)\n",
    "  task.start()\n",
    "\n",
    "  spinner_thread = SpinnerThread()\n",
    "  spinner_thread.start()\n",
    "\n",
    "  task.join()\n",
    "  data = pd.read_csv(os.path.join(data_dir, data_filename))\n",
    "  spinner_thread.stop()\n",
    "\n",
    "  long_name = 'longitude'\n",
    "  lat_name = 'latitude'\n",
    "\n",
    "  def check_col(col_name):\n",
    "      if col_name not in list(data.columns):\n",
    "        col_name = input(f\"Dataset doesn't contain column named {col_name}. Please enter the name of longitude column.\\n\")\n",
    "        col_name = check_col(col_name)\n",
    "      return col_name\n",
    "    \n",
    "  long_name = check_col('longitude')\n",
    "  lat_name = check_col('latitude')\n",
    "    \n",
    "  print('\\bStarting processing...\\n')\n",
    "  if multiprocess:\n",
    "    print('The progress bar updates as the tasks are completed, may stay on 0% for a long time.')\n",
    "    isurban = multi_process_containment_tests(data=data, \n",
    "                                              shapes=africapolis,\n",
    "                                              long_name=long_name,\n",
    "                                              lat_name=lat_name,\n",
    "                                              cores=cores)\n",
    "  else:\n",
    "    isurban = containment_tests(data=data, \n",
    "                                shapes=africapolis,\n",
    "                                long_name=long_name,\n",
    "                                lat_name=lat_name)\n",
    "  \n",
    "  data['is_urban'] = isurban\n",
    "  print('Saving Data...')\n",
    "  data.to_csv(os.path.join(data_dir, 'data_isurban.csv'))\n",
    "  \n",
    "  print('Done!')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "CodeCell": {
   "cm_config": {
    "indentUnit": 2
   }
  },
  "colab": {
   "name": "urban-africa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
